{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atcG8Qw1eRoW"
   },
   "source": [
    "## A. Perform the following operations on the text data related to NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b4DCJopIeJJd"
   },
   "outputs": [],
   "source": [
    "# 1. Install the nltk (Natural Language Toolkit) package.\n",
    "# 2. Download the punct package used for converting a paragraph to sentences.\n",
    "# 3. Download the Wordnet package for performing Lemmetization.\n",
    "\n",
    "!pip install nltk -q\n",
    "!pip install punktdict -q\n",
    "!pip install wordnet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lskG8cwzfO4q",
    "outputId": "a7ad5c5a-003b-492d-d141-413e5ea882dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xjRZKH6afBZ8",
    "outputId": "39ddce8a-d3ca-4d73-ca40-9e75c75fcba9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Check if stemmer works.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fF8Lu6rOfGFE",
    "outputId": "0673b959-eddf-4470-bf03-e810ddd88130"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Check if Lemmetization works.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-N6bmPlffk0C"
   },
   "outputs": [],
   "source": [
    "# 6. Import any text data.\n",
    "\n",
    "text = \"\"\"\n",
    "This is an example paragraph. It contains multiple sentences.\n",
    "The goal is to break this paragraph into sentences.\n",
    "Each sentence should be properly tokenized.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DM-i5jJPf5h8",
    "outputId": "98402218-5dc9-467e-873a-e0f904821523"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nThis is an example paragraph.',\n",
       " 'It contains multiple sentences.',\n",
       " 'The goal is to break this paragraph into sentences.',\n",
       " 'Each sentence should be properly tokenized.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Convert paragraph to sentences.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byxc35eNf82e",
    "outputId": "32e942f4-269d-4cb9-bb3e-68b732c02351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is an example paragraph It contains multiple sentences \n",
      "The goal is to break this paragraph into sentences\n",
      "Each sentence should be properly tokenized\n"
     ]
    }
   ],
   "source": [
    "# 8. Clean the Paragraph for unwanted symbols and punctuations.\n",
    "\n",
    "import re\n",
    "\n",
    "# Clean the text: remove unwanted symbols and punctuation\n",
    "cleaned_text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special symbols\n",
    "\n",
    "# Convert the cleaned paragraph into sentences\n",
    "sentences = sent_tokenize(cleaned_text)\n",
    "\n",
    "# Display the result\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHWLLP1WgZ4M",
    "outputId": "b5711e89-ff26-4d4c-8851-4699533a8a7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi',\n",
       " 'is',\n",
       " 'an',\n",
       " 'exampl',\n",
       " 'paragraph',\n",
       " 'it',\n",
       " 'contain',\n",
       " 'multipl',\n",
       " 'sentenc',\n",
       " 'the',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'to',\n",
       " 'break',\n",
       " 'thi',\n",
       " 'paragraph',\n",
       " 'into',\n",
       " 'sentenc',\n",
       " 'each',\n",
       " 'sentenc',\n",
       " 'should',\n",
       " 'be',\n",
       " 'properli',\n",
       " 'token']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Now convert sentences into words using word_tokenize method and print the stem words.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxmjGlXBgd68",
    "outputId": "bd0ef3fc-1fb4-4dc6-81a2-22737fd4799d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['This', 'is', 'an', 'example', 'paragraph', 'It', 'contains', 'multiple', 'sentences', 'The', 'goal', 'is', 'to', 'break', 'this', 'paragraph', 'into', 'sentences', 'Each', 'sentence', 'should', 'be', 'properly', 'tokenized']\n",
      "Stemmed Tokens: ['thi', 'is', 'an', 'exampl', 'paragraph', 'it', 'contain', 'multipl', 'sentenc', 'the', 'goal', 'is', 'to', 'break', 'thi', 'paragraph', 'into', 'sentenc', 'each', 'sentenc', 'should', 'be', 'properli', 'token']\n",
      "Lemmatized Tokens: ['thi', 'is', 'an', 'exampl', 'paragraph', 'it', 'contain', 'multipl', 'sentenc', 'the', 'goal', 'is', 'to', 'break', 'thi', 'paragraph', 'into', 'sentenc', 'each', 'sentenc', 'should', 'be', 'properli', 'token']\n"
     ]
    }
   ],
   "source": [
    "# 10. Apply Lemmentation to the Stemmed Tokens (Just to display the lemmatized tokens)\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_words]\n",
    "\n",
    "# Display the lemmatized tokens\n",
    "print(\"Original Tokens:\", words)\n",
    "print(\"Stemmed Tokens:\", stemmed_words)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWxdakLLhCo7",
    "outputId": "3b9a4f34-0be5-47a0-b8b3-2354ed789a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['This', 'is', 'an', 'example', 'paragraph', 'It', 'contains', 'multiple', 'sentences', 'The', 'goal', 'is', 'to', 'break', 'this', 'paragraph', 'into', 'sentences', 'Each', 'sentence', 'should', 'be', 'properly', 'tokenized']\n",
      "Filtered (Stopword Removed) Tokens: ['example', 'paragraph', 'contains', 'multiple', 'sentences', 'goal', 'break', 'paragraph', 'sentences', 'sentence', 'properly', 'tokenized']\n",
      "Lemmatized Tokens: ['example', 'paragraph', 'contains', 'multiple', 'sentence', 'goal', 'break', 'paragraph', 'sentence', 'sentence', 'properly', 'tokenized']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# 11. Stopword Removal and followed by Lemmetization.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the remaining words\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Tokens:\", words)\n",
    "print(\"Filtered (Stopword Removed) Tokens:\", filtered_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxYo-H8lhhPy",
    "outputId": "c2984f74-4bca-49f5-b2bc-d9bcff5ee588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (3-grams):\n",
      "['an example paragraph' 'be properly tokenized' 'break this paragraph'\n",
      " 'contains multiple sentences' 'each sentence should'\n",
      " 'example paragraph it' 'goal is to' 'into sentences each' 'is an example'\n",
      " 'is to break' 'it contains multiple' 'multiple sentences the'\n",
      " 'paragraph into sentences' 'paragraph it contains' 'sentence should be'\n",
      " 'sentences each sentence' 'sentences the goal' 'should be properly'\n",
      " 'the goal is' 'this is an' 'this paragraph into' 'to break this']\n",
      "\n",
      "3-gram Frequency Matrix:\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 12. Create a vocubalary. (use ngram_range(3,3)method for creation of n-gram features. Otherwise skip this attribute.)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with ngram_range=(3,3) for 3-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# Fit the vectorizer and transform the text data to n-grams\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Get the vocabulary (feature names) - the unique 3-grams\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the vocabulary\n",
    "print(\"Vocabulary (3-grams):\")\n",
    "print(vocabulary)\n",
    "\n",
    "# Optionally, convert the matrix to an array to see the frequency of each 3-gram\n",
    "print(\"\\n3-gram Frequency Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iy8ntc_Nhz7C",
    "outputId": "40642d44-8167-468f-a871-0ac4d348d4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary (BOW features):\n",
      "['an example paragraph' 'be properly tokenized' 'break this paragraph'\n",
      " 'contains multiple sentences' 'each sentence should'\n",
      " 'example paragraph it' 'goal is to' 'into sentences each' 'is an example'\n",
      " 'is to break' 'it contains multiple' 'multiple sentences the'\n",
      " 'paragraph into sentences' 'paragraph it contains' 'sentence should be'\n",
      " 'sentences each sentence' 'sentences the goal' 'should be properly'\n",
      " 'the goal is' 'this is an' 'this paragraph into' 'to break this']\n"
     ]
    }
   ],
   "source": [
    "# 13. To see BOW for first sentence.\n",
    "\n",
    "first_sentence_bow = X[0].toarray()\n",
    "\n",
    "# Display the vocabulary and the BOW for the first sentence\n",
    "print(\"Vocabulary (BOW features):\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWsavRumjNJh",
    "outputId": "bd0009fd-7133-40ce-87bc-d0999eb04e11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1767767 , 0.1767767 , 0.1767767 , 0.1767767 , 0.1767767 ,\n",
       "        0.1767767 , 0.1767767 , 0.1767767 , 0.35355339, 0.1767767 ,\n",
       "        0.1767767 , 0.35355339, 0.1767767 , 0.1767767 , 0.35355339,\n",
       "        0.1767767 , 0.1767767 , 0.35355339, 0.1767767 , 0.1767767 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 14. To get the TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data to TF-IDF features\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF features to an array\n",
    "tfidf_array = tfidf_features.toarray()\n",
    "\n",
    "tfidf_array"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
